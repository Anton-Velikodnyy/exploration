{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Learn as TF Estimator\n",
    "## 6 Steps to Success\n",
    "1. Find a model that fits your need\n",
    "2. Write function to import dataset\n",
    "3. Define the features from the dataset\n",
    "4. Create an instance of the model\n",
    "5. Train the model\n",
    "6. Use the trained model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpgsy29b3s\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpgsy29b3s', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f220a2f0fd0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpgsy29b3s/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.4250216, step = 1\n",
      "INFO:tensorflow:global_step/sec: 98.4118\n",
      "INFO:tensorflow:loss = 1.50372, step = 101 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4963\n",
      "INFO:tensorflow:loss = 1.0920343, step = 201 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.9964\n",
      "INFO:tensorflow:loss = 0.77381086, step = 301 (1.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.8244\n",
      "INFO:tensorflow:loss = 0.59233403, step = 401 (1.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.694\n",
      "INFO:tensorflow:loss = 0.6187978, step = 501 (0.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4933\n",
      "INFO:tensorflow:loss = 0.63639265, step = 601 (1.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.8178\n",
      "INFO:tensorflow:loss = 0.48641095, step = 701 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.654\n",
      "INFO:tensorflow:loss = 0.4463762, step = 801 (0.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3181\n",
      "INFO:tensorflow:loss = 0.32611197, step = 901 (1.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.77\n",
      "INFO:tensorflow:loss = 0.5228144, step = 1001 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.7916\n",
      "INFO:tensorflow:loss = 0.38481557, step = 1101 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8184\n",
      "INFO:tensorflow:loss = 0.4444974, step = 1201 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.6633\n",
      "INFO:tensorflow:loss = 0.43126038, step = 1301 (1.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.293\n",
      "INFO:tensorflow:loss = 0.38603637, step = 1401 (0.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.1348\n",
      "INFO:tensorflow:loss = 0.23131233, step = 1501 (1.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.677\n",
      "INFO:tensorflow:loss = 0.29553214, step = 1601 (1.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.2154\n",
      "INFO:tensorflow:loss = 0.42517883, step = 1701 (1.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6457\n",
      "INFO:tensorflow:loss = 0.335165, step = 1801 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2842\n",
      "INFO:tensorflow:loss = 0.43841147, step = 1901 (1.241 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmpgsy29b3s/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3609367.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-13-12:52:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpgsy29b3s/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-13-12:52:50\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9034, global_step = 2000, loss = 0.33638346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9034, 'global_step': 2000, 'loss': 0.33638346}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1?\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 2\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
    "                                  one_hot=False)\n",
    "# 3                                  \n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 200\n",
    "n_steps = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \"\"\" define the model function\n",
    "    \"\"\"\n",
    "    espec_op = tf.estimator.EstimatorSpec\n",
    "    # features is a dict as per Estimator specifications\n",
    "    x = features['images']\n",
    "    # define the network\n",
    "    layer_1 = tf.layers.dense(x, 32)\n",
    "    layer_2 = tf.layers.dense(layer_1, 32)\n",
    "    logits = tf.layers.dense(layer_2, n_classes)\n",
    "\n",
    "    # define predicted classes\n",
    "    predicted_classes = tf.argmax(logits, axis=1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        espec = espec_op(mode,\n",
    "                         predictions=predicted_classes\n",
    "                         )\n",
    "    else:\n",
    "        # define loss and optimizer\n",
    "        entropy_op = tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        loss_op = tf.reduce_mean(entropy_op(logits=logits,\n",
    "                                            labels=tf.cast(labels,\n",
    "                                                           dtype=tf.int32)\n",
    "                                            )\n",
    "                                 )\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # define accuracy\n",
    "        accuracy_op = tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predicted_classes)\n",
    "\n",
    "        espec = espec_op(mode=mode,\n",
    "                         predictions=predicted_classes,\n",
    "                         loss=loss_op,\n",
    "                         train_op=train_op,\n",
    "                         eval_metric_ops={'accuracy': accuracy_op}\n",
    "                         )\n",
    "\n",
    "    return espec\n",
    "\n",
    "# 4\n",
    "# create estimator object\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "# 5\n",
    "# train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': x_train},\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "model.train(train_input_fn, steps=n_steps)\n",
    "# 6\n",
    "# evaluate the model\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': x_test},\n",
    "    y=y_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Slim\n",
    "## 7 Steps to Success\n",
    "1. Create the model using slim layers\n",
    "2. Provide the input to the layers to instantiate the model\n",
    "3. Use logits and labels to define the loss\n",
    "4. Get the total loss using convience function ```get_loss_total()```\n",
    "5. Create optimizer\n",
    "6. Create a training function using convinience function ```slim.learning.create_train_op()```,```total_loss``` and ```optimizer```\n",
    "7. Run the training using the convinience function ```slim.learning.train()``` and training function defined in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/hackerman/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From /home/hackerman/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path ./slim_logs/model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 100: loss = 2.2404 (0.965 sec/step)\n",
      "INFO:tensorflow:global step 200: loss = 2.1648 (0.817 sec/step)\n",
      "INFO:tensorflow:global step 300: loss = 2.0825 (0.925 sec/step)\n",
      "INFO:tensorflow:global step 400: loss = 2.0035 (0.852 sec/step)\n",
      "INFO:tensorflow:global step 500: loss = 1.9303 (0.808 sec/step)\n",
      "INFO:tensorflow:global step 600: loss = 1.8626 (0.983 sec/step)\n",
      "INFO:tensorflow:Saving checkpoint to path ./slim_logs/model.ckpt\n",
      "INFO:tensorflow:global_step/sec: 1.15364\n",
      "INFO:tensorflow:global step 700: loss = 1.8040 (0.749 sec/step)\n",
      "INFO:tensorflow:global step 800: loss = 1.7488 (0.792 sec/step)\n",
      "INFO:tensorflow:global step 900: loss = 1.6922 (0.824 sec/step)\n",
      "INFO:tensorflow:global step 1000: loss = 1.6503 (1.148 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "final loss=1.65033757686615\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import slim\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_classes = 10  \n",
    "n_steps = 1000\n",
    "\n",
    "# let us get the data\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'), one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "Y_train = mnist.train.labels\n",
    "Y_train = tf.convert_to_tensor(Y_train)\n",
    "\n",
    "\n",
    "def mlp(x):\n",
    "    net = slim.fully_connected(x, 32, scope='fc1')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout1')\n",
    "    net = slim.fully_connected(net, 32, scope='fc2')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout2')\n",
    "    net = slim.fully_connected(net, n_classes, activation_fn=None, scope='fc3')\n",
    "    return net\n",
    "\n",
    "\n",
    "# Define the model\n",
    "logits = mlp(X_train)\n",
    "\n",
    "# Define the loss functions and get the total loss\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y_train)\n",
    "total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "# Run the training\n",
    "final_loss = slim.learning.train(\n",
    "    train_op,\n",
    "    logdir='./slim_logs',\n",
    "    number_of_steps=n_steps,\n",
    "    log_every_n_steps=100)\n",
    "\n",
    "print('final loss={}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLearn (without the space)\n",
    "## 7 Steps to Success\n",
    "1. Create an input layer\n",
    "2. Pass the object (rock) to create further layers\n",
    "3. Add the output layer (usualy fully connnected)\n",
    "4. Create the net using an estimator level such as ```regresssion```.\n",
    "5. Create a model from the net created in the previous step\n",
    "6. Train the model with the ```train.fit()``` method\n",
    "7. Use the trained model to predict or evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5499  | total loss: \u001b[1m\u001b[32m0.28319\u001b[0m\u001b[0m | time: 10.974s\n",
      "| Adam | epoch: 010 | loss: 0.28319 - acc: 0.9148 -- iter: 54900/55000\n",
      "Training Step: 5500  | total loss: \u001b[1m\u001b[32m0.27340\u001b[0m\u001b[0m | time: 10.990s\n",
      "| Adam | epoch: 010 | loss: 0.27340 - acc: 0.9173 -- iter: 55000/55000\n",
      "--\n",
      "Test accuracy: 0.9109\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "import os\n",
    "\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "n_epochs = 10\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist.load_data(\n",
    "    data_dir=os.path.join('.', 'mnist'), one_hot=True)\n",
    "\n",
    "# Build deep neural network\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "layer1 = tflearn.fully_connected(input_layer,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n",
    "layer2 = tflearn.fully_connected(layer1,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n",
    "output = tflearn.fully_connected(layer2,\n",
    "                                 n_classes,\n",
    "                                 activation='softmax'\n",
    "                                 )\n",
    "\n",
    "net = tflearn.regression(output,\n",
    "                         optimizer='adam',\n",
    "                         metric=tflearn.metrics.Accuracy(),\n",
    "                         loss='categorical_crossentropy'\n",
    "                         )\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    n_epoch=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    show_metric=True,\n",
    "    run_id='dense_model')\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrettyTensor\n",
    "# 8 Steps to Success\n",
    "1. Get the data\n",
    "2. Define the hyperparameters and parameters\n",
    "3. Define the inputs and the outputs\n",
    "4. Define the model\n",
    "5. Define the evaluater, optimizer, and trainter functions\n",
    "6. Create the runner object\n",
    "7. Within a Tensorflow session, train the model with the ```runner.train_model()``` method\n",
    "8. Within the same session, evaluate the model with the ```runner.evaluate_model()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "[1] [2.5092697]\n",
      "[600] [0.35631377]\n",
      "Accuracy after 1 epochs 0.9099999666213989 \n",
      "\n",
      "[601] [0.25389308]\n",
      "[1200] [0.2711808]\n",
      "Accuracy after 2 epochs 0.8899999856948853 \n",
      "\n",
      "[1201] [0.5084967]\n",
      "[1800] [0.20958649]\n",
      "Accuracy after 3 epochs 0.8999999761581421 \n",
      "\n",
      "[1801] [0.23753788]\n",
      "[2400] [0.34706223]\n",
      "Accuracy after 4 epochs 0.8899999856948853 \n",
      "\n",
      "[2401] [0.24079672]\n",
      "[3000] [0.2273854]\n",
      "Accuracy after 5 epochs 0.8899999856948853 \n",
      "\n",
      "[3001] [0.45683563]\n",
      "[3600] [0.2745242]\n",
      "Accuracy after 6 epochs 0.8999999761581421 \n",
      "\n",
      "[3601] [0.35685435]\n",
      "[4200] [0.44169852]\n",
      "Accuracy after 7 epochs 0.8999999761581421 \n",
      "\n",
      "[4201] [0.2994276]\n",
      "[4800] [0.2605332]\n",
      "Accuracy after 8 epochs 0.8899999856948853 \n",
      "\n",
      "[4801] [0.1798949]\n",
      "[5400] [0.22237508]\n",
      "Accuracy after 9 epochs 0.8999999761581421 \n",
      "\n",
      "[5401] [0.15331356]\n",
      "[6000] [0.315279]\n",
      "Accuracy after 10 epochs 0.8899999856948853 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "import prettytensor as pt\n",
    "from prettytensor.tutorial import data_utils\n",
    "import os\n",
    "data_utils.WORK_DIRECTORY = os.path.join('.', 'mnist')\n",
    "\n",
    "# get the data\n",
    "X_train, Y_train = data_utils.mnist(training=True)\n",
    "X_test, Y_test = data_utils.mnist(training=False)\n",
    "\n",
    "# define hyperparameters\n",
    "\n",
    "batch_size = 100     # number of samples that would\n",
    "                     # be used to learn the parameters in a batch\n",
    "n_classes = 10       # number of outputs, i.e. digits 0 to 9\n",
    "n_epochs = 10        # number of ietrations for learning the parameters\n",
    "n_batches = int(X_train.shape[0] / batch_size)\n",
    "n_samples_in_train_batch = 60000 // batch_size\n",
    "n_samples_in_test_batch = 10000 // batch_size\n",
    "\n",
    "# define inputs and outputs\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "\n",
    "# define the model\n",
    "\n",
    "X = pt.wrap(X)\n",
    "\n",
    "model = (X.\n",
    "         flatten().\n",
    "         fully_connected(10).\n",
    "         softmax_classifier(n_classes, labels=Y)\n",
    "         )\n",
    "\n",
    "# define evaluator (metrics), optimizer and training functions\n",
    "\n",
    "evaluator = model.softmax.evaluate_classifier(Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "trainer = pt.apply_optimizer(optimizer, losses=[model.loss])\n",
    "\n",
    "runner = pt.train.Runner()\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    for epoch in range(0, n_epochs):\n",
    "        # shuffle the training data\n",
    "        X_train, Y_train = data_utils.permute_data((X_train, Y_train))\n",
    "\n",
    "        runner.train_model(\n",
    "            trainer,\n",
    "            model.loss,\n",
    "            n_samples_in_train_batch,\n",
    "            feed_vars=(X, Y),\n",
    "            feed_data=pt.train.feed_numpy(batch_size, X_train, Y_train),\n",
    "            print_every=600\n",
    "        )\n",
    "\n",
    "        score = runner.evaluate_model(\n",
    "            evaluator,\n",
    "            n_samples_in_test_batch,\n",
    "            feed_vars=(X, Y),\n",
    "            feed_data=pt.train.feed_numpy(batch_size, X_test, Y_test)\n",
    "        )\n",
    "\n",
    "        print('Accuracy after {} epochs {} \\n'.\n",
    "              format(epoch + 1, score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonnet\n",
    "## 8 Steps to Success\n",
    "1. Create classes for the dataset and network architecture which inherit from ```sonnet.AbstractModuel```. In this example, I have a MNIST class and an MLP class\n",
    "2. Define the hyperparameters and parameters\n",
    "3. Define the test and the train datasets from the dataset class defined in the proceeding step.\n",
    "4. Define the model using the network class defined. As an example, ```model = MLP([20, n_classes])``` in this case creates an MLP network with two layers for 20 and the ```n_classes``` number of neurons each\n",
    "5. Define the ```y_hat``` placeholders for the train and test set using the model\n",
    "6. Define the loss placeholders for the train and test sets\n",
    "7. Define the optimizer using the train loss placeholder\n",
    "8. Execute the loss function in a Tensorflow session for the desired number of epochs to optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch : 0 Training Loss : 235.41871643066406\n",
      "Epoch : 1 Training Loss : 228.8345184326172\n",
      "Epoch : 2 Training Loss : 222.725341796875\n",
      "Epoch : 3 Training Loss : 223.00149536132812\n",
      "Epoch : 4 Training Loss : 213.0349884033203\n",
      "Epoch : 5 Training Loss : 209.04672241210938\n",
      "Epoch : 6 Training Loss : 209.91319274902344\n",
      "Epoch : 7 Training Loss : 200.43603515625\n",
      "Epoch : 8 Training Loss : 203.97579956054688\n",
      "Epoch : 9 Training Loss : 189.20249938964844\n",
      "Test loss : 186.88563537597656\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import os\n",
    "import sonnet as snt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "class MNIST(snt.AbstractModule):\n",
    "\n",
    "    def __init__(self, mnist_part, batch_size, name='MNIST'):\n",
    "\n",
    "        super(MNIST, self).__init__(name=name)\n",
    "\n",
    "        self._X = tf.constant(mnist_part.images, dtype=tf.float32)\n",
    "        self._Y = tf.constant(mnist_part.labels, dtype=tf.float32)\n",
    "        self._batch_size = batch_size\n",
    "        self._M = mnist_part.num_examples\n",
    "\n",
    "    def _build(self):\n",
    "        idx = tf.random_uniform([self._batch_size], 0, self._M, tf.int64)\n",
    "        X = tf.gather(self._X, idx)\n",
    "        Y = tf.gather(self._Y, idx)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "class MLP(snt.AbstractModule):\n",
    "    def __init__(self, output_sizes, name='mlp'):\n",
    "        super(MLP, self).__init__(name=name)\n",
    "\n",
    "        self._layers = []\n",
    "\n",
    "        for output_size in output_sizes:\n",
    "            self._layers.append(snt.Linear(output_size=output_size))\n",
    "\n",
    "    def _build(self, X):\n",
    "\n",
    "        # add the input layer\n",
    "        model = tf.sigmoid(self._layers[0](X))\n",
    "\n",
    "        # add hidden layers\n",
    "        for i in range(1, len(self._layers) - 1):\n",
    "            model = tf.sigmoid(self._layers[i](model))\n",
    "\n",
    "        # add output layer\n",
    "        model = tf.nn.softmax(self._layers[len(self._layers) - 1](model))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "n_epochs = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
    "                                  one_hot=True\n",
    "                                  )\n",
    "train = MNIST(mnist.train, batch_size=batch_size)\n",
    "test = MNIST(mnist.test, batch_size=batch_size)\n",
    "\n",
    "X_train, Y_train = train()\n",
    "X_test, Y_test = test()\n",
    "\n",
    "model = MLP([20, n_classes])\n",
    "\n",
    "Y_train_hat = model(X_train)\n",
    "Y_test_hat = model(X_test)\n",
    "\n",
    "\n",
    "def loss(Y_hat, Y):\n",
    "    return -tf.reduce_sum(Y * tf.log(Y_hat))\n",
    "\n",
    "\n",
    "L_train = loss(Y_train_hat, Y_train)\n",
    "L_test = loss(Y_test_hat, Y_test)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.01).minimize(L_train)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_val, _ = tfs.run((L_train, optimizer))\n",
    "        print('Epoch : {} Training Loss : {}'.format(epoch, loss_val))\n",
    "\n",
    "    loss_val = tfs.run(L_test)\n",
    "    print('Test loss : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
